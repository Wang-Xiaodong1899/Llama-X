{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2e5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404d568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(os.path.join('/f_data/G', \"Emu/Emu/Emu-instruct.pt\"), map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "612698b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dicts = OrderedDict()\n",
    "    \n",
    "# directly load, visual, ln_visual, cformer\n",
    "for k, v in ckpt.items():\n",
    "    if 'decoder.lm.base_model.model.model' in k: # embed_tokens, layers\n",
    "        new_state_dicts[k.replace('decoder.lm.base_model.model.model', 'base_model.model.model')] = v\n",
    "    elif 'decoder.lm.base_model.model.lm_head' in k: # lm_head\n",
    "        new_state_dicts[k.replace('decoder.lm.base_model.model.lm_head', 'base_model.model.lm_head')] = v\n",
    "    elif 'decoder.lm.base_model.model.stu_regress_head.weight' in k: # stu_regress_head\n",
    "        new_state_dicts['base_model.model.stu_regress_head.weight'] = v\n",
    "    else:\n",
    "        new_state_dicts['base_model.model.'+k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bca14666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Llama-X')\n",
    "from src.train_emu_t2i import *\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c6b7552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /workspace/Llama-X/emu/Emu-14B.json\n"
     ]
    }
   ],
   "source": [
    "args = llamaconfig()\n",
    "\n",
    "config = LlamaConfig.from_pretrained('/f_data/G/llama/llama-13b-hf/')\n",
    "emu_config = file2data('/workspace/Llama-X/emu/Emu-14B.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "764c69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelArguments = ModelArguments(model_name_or_path='/f_data/G/llama/llama-13b-hf/')\n",
    "dataArguments = DataArguments()\n",
    "trainingArguments = TrainingArguments(output_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1793baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Image Encoder\n"
     ]
    }
   ],
   "source": [
    "model = LlamaNUWA(config=config, **emu_config, cast_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec60a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_BOS_TOKEN = '<s>'\n",
    "DEFAULT_EOS_TOKEN = '</s>'\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "DEFAULT_IMG_START_TOKEN = \"[IMG]\"\n",
    "DEFAULT_IMG_END_TOKEN = \"[/IMG]\"\n",
    "DEFAULT_IMG_TOKEN = \"<image>\"\n",
    "USER_TOKEN = '[USER]'\n",
    "ASSISTANT_TOKEN = '[ASSISTANT]'\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\n",
    "        '/f_data/G/llama/llama-13b-hf/',\n",
    "        model_max_length=512,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "special_token_list = [DEFAULT_IMG_START_TOKEN, DEFAULT_IMG_END_TOKEN, DEFAULT_IMG_TOKEN, USER_TOKEN,\n",
    "                              ASSISTANT_TOKEN]\n",
    "special_tokens_dict = dict(\n",
    "        pad_token=DEFAULT_PAD_TOKEN,\n",
    "        bos_token=DEFAULT_BOS_TOKEN,\n",
    "        eos_token=DEFAULT_EOS_TOKEN,\n",
    "        unk_token=DEFAULT_UNK_TOKEN,\n",
    "        additional_special_tokens=special_token_list\n",
    "    )\n",
    "if tokenizer.pad_token is None:\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=special_tokens_dict,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba525244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21abbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of state_dict were not used in target: ['base_model.model.stu_regress_head.weight']\n"
     ]
    }
   ],
   "source": [
    "adaptively_load_state_dict(model, new_state_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de4c342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.layers.32.self_attn.q_proj.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.k_proj.weight', 'base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.v_proj.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.o_proj.weight', 'base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.32.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.32.mlp.gate_proj.weight', 'base_model.model.model.layers.32.mlp.down_proj.weight', 'base_model.model.model.layers.32.mlp.up_proj.weight', 'base_model.model.model.layers.32.input_layernorm.weight', 'base_model.model.model.layers.32.post_attention_layernorm.weight', 'base_model.model.model.layers.33.self_attn.q_proj.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.k_proj.weight', 'base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.v_proj.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.o_proj.weight', 'base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.33.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.33.mlp.gate_proj.weight', 'base_model.model.model.layers.33.mlp.down_proj.weight', 'base_model.model.model.layers.33.mlp.up_proj.weight', 'base_model.model.model.layers.33.input_layernorm.weight', 'base_model.model.model.layers.33.post_attention_layernorm.weight', 'base_model.model.model.layers.34.self_attn.q_proj.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.k_proj.weight', 'base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.v_proj.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.o_proj.weight', 'base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.34.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.34.mlp.gate_proj.weight', 'base_model.model.model.layers.34.mlp.down_proj.weight', 'base_model.model.model.layers.34.mlp.up_proj.weight', 'base_model.model.model.layers.34.input_layernorm.weight', 'base_model.model.model.layers.34.post_attention_layernorm.weight', 'base_model.model.model.layers.35.self_attn.q_proj.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.k_proj.weight', 'base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.v_proj.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.o_proj.weight', 'base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.35.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.35.mlp.gate_proj.weight', 'base_model.model.model.layers.35.mlp.down_proj.weight', 'base_model.model.model.layers.35.mlp.up_proj.weight', 'base_model.model.model.layers.35.input_layernorm.weight', 'base_model.model.model.layers.35.post_attention_layernorm.weight', 'base_model.model.model.layers.36.self_attn.q_proj.weight', 'base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.k_proj.weight', 'base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.v_proj.weight', 'base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.o_proj.weight', 'base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.36.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.36.mlp.gate_proj.weight', 'base_model.model.model.layers.36.mlp.down_proj.weight', 'base_model.model.model.layers.36.mlp.up_proj.weight', 'base_model.model.model.layers.36.input_layernorm.weight', 'base_model.model.model.layers.36.post_attention_layernorm.weight', 'base_model.model.model.layers.37.self_attn.q_proj.weight', 'base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.k_proj.weight', 'base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.v_proj.weight', 'base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.o_proj.weight', 'base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.37.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.37.mlp.gate_proj.weight', 'base_model.model.model.layers.37.mlp.down_proj.weight', 'base_model.model.model.layers.37.mlp.up_proj.weight', 'base_model.model.model.layers.37.input_layernorm.weight', 'base_model.model.model.layers.37.post_attention_layernorm.weight', 'base_model.model.model.layers.38.self_attn.q_proj.weight', 'base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.k_proj.weight', 'base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.v_proj.weight', 'base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.o_proj.weight', 'base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.38.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.38.mlp.gate_proj.weight', 'base_model.model.model.layers.38.mlp.down_proj.weight', 'base_model.model.model.layers.38.mlp.up_proj.weight', 'base_model.model.model.layers.38.input_layernorm.weight', 'base_model.model.model.layers.38.post_attention_layernorm.weight', 'base_model.model.model.layers.39.self_attn.q_proj.weight', 'base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.k_proj.weight', 'base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.v_proj.weight', 'base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.o_proj.weight', 'base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.39.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.39.mlp.gate_proj.weight', 'base_model.model.model.layers.39.mlp.down_proj.weight', 'base_model.model.model.layers.39.mlp.up_proj.weight', 'base_model.model.model.layers.39.input_layernorm.weight', 'base_model.model.model.layers.39.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight', 'base_model.model.visual.cls_token', 'base_model.model.visual.pos_embed', 'base_model.model.visual.patch_embed.proj.weight', 'base_model.model.visual.patch_embed.proj.bias', 'base_model.model.visual.blocks.0.norm1.weight', 'base_model.model.visual.blocks.0.norm1.bias', 'base_model.model.visual.blocks.0.attn.q_bias', 'base_model.model.visual.blocks.0.attn.v_bias', 'base_model.model.visual.blocks.0.attn.qkv.weight', 'base_model.model.visual.blocks.0.attn.proj.weight', 'base_model.model.visual.blocks.0.attn.proj.bias', 'base_model.model.visual.blocks.0.norm2.weight', 'base_model.model.visual.blocks.0.norm2.bias', 'base_model.model.visual.blocks.0.mlp.fc1.weight', 'base_model.model.visual.blocks.0.mlp.fc1.bias', 'base_model.model.visual.blocks.0.mlp.fc2.weight', 'base_model.model.visual.blocks.0.mlp.fc2.bias', 'base_model.model.visual.blocks.1.norm1.weight', 'base_model.model.visual.blocks.1.norm1.bias', 'base_model.model.visual.blocks.1.attn.q_bias', 'base_model.model.visual.blocks.1.attn.v_bias', 'base_model.model.visual.blocks.1.attn.qkv.weight', 'base_model.model.visual.blocks.1.attn.proj.weight', 'base_model.model.visual.blocks.1.attn.proj.bias', 'base_model.model.visual.blocks.1.norm2.weight', 'base_model.model.visual.blocks.1.norm2.bias', 'base_model.model.visual.blocks.1.mlp.fc1.weight', 'base_model.model.visual.blocks.1.mlp.fc1.bias', 'base_model.model.visual.blocks.1.mlp.fc2.weight', 'base_model.model.visual.blocks.1.mlp.fc2.bias', 'base_model.model.visual.blocks.2.norm1.weight', 'base_model.model.visual.blocks.2.norm1.bias', 'base_model.model.visual.blocks.2.attn.q_bias', 'base_model.model.visual.blocks.2.attn.v_bias', 'base_model.model.visual.blocks.2.attn.qkv.weight', 'base_model.model.visual.blocks.2.attn.proj.weight', 'base_model.model.visual.blocks.2.attn.proj.bias', 'base_model.model.visual.blocks.2.norm2.weight', 'base_model.model.visual.blocks.2.norm2.bias', 'base_model.model.visual.blocks.2.mlp.fc1.weight', 'base_model.model.visual.blocks.2.mlp.fc1.bias', 'base_model.model.visual.blocks.2.mlp.fc2.weight', 'base_model.model.visual.blocks.2.mlp.fc2.bias', 'base_model.model.visual.blocks.3.norm1.weight', 'base_model.model.visual.blocks.3.norm1.bias', 'base_model.model.visual.blocks.3.attn.q_bias', 'base_model.model.visual.blocks.3.attn.v_bias', 'base_model.model.visual.blocks.3.attn.qkv.weight', 'base_model.model.visual.blocks.3.attn.proj.weight', 'base_model.model.visual.blocks.3.attn.proj.bias', 'base_model.model.visual.blocks.3.norm2.weight', 'base_model.model.visual.blocks.3.norm2.bias', 'base_model.model.visual.blocks.3.mlp.fc1.weight', 'base_model.model.visual.blocks.3.mlp.fc1.bias', 'base_model.model.visual.blocks.3.mlp.fc2.weight', 'base_model.model.visual.blocks.3.mlp.fc2.bias', 'base_model.model.visual.blocks.4.norm1.weight', 'base_model.model.visual.blocks.4.norm1.bias', 'base_model.model.visual.blocks.4.attn.q_bias', 'base_model.model.visual.blocks.4.attn.v_bias', 'base_model.model.visual.blocks.4.attn.qkv.weight', 'base_model.model.visual.blocks.4.attn.proj.weight', 'base_model.model.visual.blocks.4.attn.proj.bias', 'base_model.model.visual.blocks.4.norm2.weight', 'base_model.model.visual.blocks.4.norm2.bias', 'base_model.model.visual.blocks.4.mlp.fc1.weight', 'base_model.model.visual.blocks.4.mlp.fc1.bias', 'base_model.model.visual.blocks.4.mlp.fc2.weight', 'base_model.model.visual.blocks.4.mlp.fc2.bias', 'base_model.model.visual.blocks.5.norm1.weight', 'base_model.model.visual.blocks.5.norm1.bias', 'base_model.model.visual.blocks.5.attn.q_bias', 'base_model.model.visual.blocks.5.attn.v_bias', 'base_model.model.visual.blocks.5.attn.qkv.weight', 'base_model.model.visual.blocks.5.attn.proj.weight', 'base_model.model.visual.blocks.5.attn.proj.bias', 'base_model.model.visual.blocks.5.norm2.weight', 'base_model.model.visual.blocks.5.norm2.bias', 'base_model.model.visual.blocks.5.mlp.fc1.weight', 'base_model.model.visual.blocks.5.mlp.fc1.bias', 'base_model.model.visual.blocks.5.mlp.fc2.weight', 'base_model.model.visual.blocks.5.mlp.fc2.bias', 'base_model.model.visual.blocks.6.norm1.weight', 'base_model.model.visual.blocks.6.norm1.bias', 'base_model.model.visual.blocks.6.attn.q_bias', 'base_model.model.visual.blocks.6.attn.v_bias', 'base_model.model.visual.blocks.6.attn.qkv.weight', 'base_model.model.visual.blocks.6.attn.proj.weight', 'base_model.model.visual.blocks.6.attn.proj.bias', 'base_model.model.visual.blocks.6.norm2.weight', 'base_model.model.visual.blocks.6.norm2.bias', 'base_model.model.visual.blocks.6.mlp.fc1.weight', 'base_model.model.visual.blocks.6.mlp.fc1.bias', 'base_model.model.visual.blocks.6.mlp.fc2.weight', 'base_model.model.visual.blocks.6.mlp.fc2.bias', 'base_model.model.visual.blocks.7.norm1.weight', 'base_model.model.visual.blocks.7.norm1.bias', 'base_model.model.visual.blocks.7.attn.q_bias', 'base_model.model.visual.blocks.7.attn.v_bias', 'base_model.model.visual.blocks.7.attn.qkv.weight', 'base_model.model.visual.blocks.7.attn.proj.weight', 'base_model.model.visual.blocks.7.attn.proj.bias', 'base_model.model.visual.blocks.7.norm2.weight', 'base_model.model.visual.blocks.7.norm2.bias', 'base_model.model.visual.blocks.7.mlp.fc1.weight', 'base_model.model.visual.blocks.7.mlp.fc1.bias', 'base_model.model.visual.blocks.7.mlp.fc2.weight', 'base_model.model.visual.blocks.7.mlp.fc2.bias', 'base_model.model.visual.blocks.8.norm1.weight', 'base_model.model.visual.blocks.8.norm1.bias', 'base_model.model.visual.blocks.8.attn.q_bias', 'base_model.model.visual.blocks.8.attn.v_bias', 'base_model.model.visual.blocks.8.attn.qkv.weight', 'base_model.model.visual.blocks.8.attn.proj.weight', 'base_model.model.visual.blocks.8.attn.proj.bias', 'base_model.model.visual.blocks.8.norm2.weight', 'base_model.model.visual.blocks.8.norm2.bias', 'base_model.model.visual.blocks.8.mlp.fc1.weight', 'base_model.model.visual.blocks.8.mlp.fc1.bias', 'base_model.model.visual.blocks.8.mlp.fc2.weight', 'base_model.model.visual.blocks.8.mlp.fc2.bias', 'base_model.model.visual.blocks.9.norm1.weight', 'base_model.model.visual.blocks.9.norm1.bias', 'base_model.model.visual.blocks.9.attn.q_bias', 'base_model.model.visual.blocks.9.attn.v_bias', 'base_model.model.visual.blocks.9.attn.qkv.weight', 'base_model.model.visual.blocks.9.attn.proj.weight', 'base_model.model.visual.blocks.9.attn.proj.bias', 'base_model.model.visual.blocks.9.norm2.weight', 'base_model.model.visual.blocks.9.norm2.bias', 'base_model.model.visual.blocks.9.mlp.fc1.weight', 'base_model.model.visual.blocks.9.mlp.fc1.bias', 'base_model.model.visual.blocks.9.mlp.fc2.weight', 'base_model.model.visual.blocks.9.mlp.fc2.bias', 'base_model.model.visual.blocks.10.norm1.weight', 'base_model.model.visual.blocks.10.norm1.bias', 'base_model.model.visual.blocks.10.attn.q_bias', 'base_model.model.visual.blocks.10.attn.v_bias', 'base_model.model.visual.blocks.10.attn.qkv.weight', 'base_model.model.visual.blocks.10.attn.proj.weight', 'base_model.model.visual.blocks.10.attn.proj.bias', 'base_model.model.visual.blocks.10.norm2.weight', 'base_model.model.visual.blocks.10.norm2.bias', 'base_model.model.visual.blocks.10.mlp.fc1.weight', 'base_model.model.visual.blocks.10.mlp.fc1.bias', 'base_model.model.visual.blocks.10.mlp.fc2.weight', 'base_model.model.visual.blocks.10.mlp.fc2.bias', 'base_model.model.visual.blocks.11.norm1.weight', 'base_model.model.visual.blocks.11.norm1.bias', 'base_model.model.visual.blocks.11.attn.q_bias', 'base_model.model.visual.blocks.11.attn.v_bias', 'base_model.model.visual.blocks.11.attn.qkv.weight', 'base_model.model.visual.blocks.11.attn.proj.weight', 'base_model.model.visual.blocks.11.attn.proj.bias', 'base_model.model.visual.blocks.11.norm2.weight', 'base_model.model.visual.blocks.11.norm2.bias', 'base_model.model.visual.blocks.11.mlp.fc1.weight', 'base_model.model.visual.blocks.11.mlp.fc1.bias', 'base_model.model.visual.blocks.11.mlp.fc2.weight', 'base_model.model.visual.blocks.11.mlp.fc2.bias', 'base_model.model.visual.blocks.12.norm1.weight', 'base_model.model.visual.blocks.12.norm1.bias', 'base_model.model.visual.blocks.12.attn.q_bias', 'base_model.model.visual.blocks.12.attn.v_bias', 'base_model.model.visual.blocks.12.attn.qkv.weight', 'base_model.model.visual.blocks.12.attn.proj.weight', 'base_model.model.visual.blocks.12.attn.proj.bias', 'base_model.model.visual.blocks.12.norm2.weight', 'base_model.model.visual.blocks.12.norm2.bias', 'base_model.model.visual.blocks.12.mlp.fc1.weight', 'base_model.model.visual.blocks.12.mlp.fc1.bias', 'base_model.model.visual.blocks.12.mlp.fc2.weight', 'base_model.model.visual.blocks.12.mlp.fc2.bias', 'base_model.model.visual.blocks.13.norm1.weight', 'base_model.model.visual.blocks.13.norm1.bias', 'base_model.model.visual.blocks.13.attn.q_bias', 'base_model.model.visual.blocks.13.attn.v_bias', 'base_model.model.visual.blocks.13.attn.qkv.weight', 'base_model.model.visual.blocks.13.attn.proj.weight', 'base_model.model.visual.blocks.13.attn.proj.bias', 'base_model.model.visual.blocks.13.norm2.weight', 'base_model.model.visual.blocks.13.norm2.bias', 'base_model.model.visual.blocks.13.mlp.fc1.weight', 'base_model.model.visual.blocks.13.mlp.fc1.bias', 'base_model.model.visual.blocks.13.mlp.fc2.weight', 'base_model.model.visual.blocks.13.mlp.fc2.bias', 'base_model.model.visual.blocks.14.norm1.weight', 'base_model.model.visual.blocks.14.norm1.bias', 'base_model.model.visual.blocks.14.attn.q_bias', 'base_model.model.visual.blocks.14.attn.v_bias', 'base_model.model.visual.blocks.14.attn.qkv.weight', 'base_model.model.visual.blocks.14.attn.proj.weight', 'base_model.model.visual.blocks.14.attn.proj.bias', 'base_model.model.visual.blocks.14.norm2.weight', 'base_model.model.visual.blocks.14.norm2.bias', 'base_model.model.visual.blocks.14.mlp.fc1.weight', 'base_model.model.visual.blocks.14.mlp.fc1.bias', 'base_model.model.visual.blocks.14.mlp.fc2.weight', 'base_model.model.visual.blocks.14.mlp.fc2.bias', 'base_model.model.visual.blocks.15.norm1.weight', 'base_model.model.visual.blocks.15.norm1.bias', 'base_model.model.visual.blocks.15.attn.q_bias', 'base_model.model.visual.blocks.15.attn.v_bias', 'base_model.model.visual.blocks.15.attn.qkv.weight', 'base_model.model.visual.blocks.15.attn.proj.weight', 'base_model.model.visual.blocks.15.attn.proj.bias', 'base_model.model.visual.blocks.15.norm2.weight', 'base_model.model.visual.blocks.15.norm2.bias', 'base_model.model.visual.blocks.15.mlp.fc1.weight', 'base_model.model.visual.blocks.15.mlp.fc1.bias', 'base_model.model.visual.blocks.15.mlp.fc2.weight', 'base_model.model.visual.blocks.15.mlp.fc2.bias', 'base_model.model.visual.blocks.16.norm1.weight', 'base_model.model.visual.blocks.16.norm1.bias', 'base_model.model.visual.blocks.16.attn.q_bias', 'base_model.model.visual.blocks.16.attn.v_bias', 'base_model.model.visual.blocks.16.attn.qkv.weight', 'base_model.model.visual.blocks.16.attn.proj.weight', 'base_model.model.visual.blocks.16.attn.proj.bias', 'base_model.model.visual.blocks.16.norm2.weight', 'base_model.model.visual.blocks.16.norm2.bias', 'base_model.model.visual.blocks.16.mlp.fc1.weight', 'base_model.model.visual.blocks.16.mlp.fc1.bias', 'base_model.model.visual.blocks.16.mlp.fc2.weight', 'base_model.model.visual.blocks.16.mlp.fc2.bias', 'base_model.model.visual.blocks.17.norm1.weight', 'base_model.model.visual.blocks.17.norm1.bias', 'base_model.model.visual.blocks.17.attn.q_bias', 'base_model.model.visual.blocks.17.attn.v_bias', 'base_model.model.visual.blocks.17.attn.qkv.weight', 'base_model.model.visual.blocks.17.attn.proj.weight', 'base_model.model.visual.blocks.17.attn.proj.bias', 'base_model.model.visual.blocks.17.norm2.weight', 'base_model.model.visual.blocks.17.norm2.bias', 'base_model.model.visual.blocks.17.mlp.fc1.weight', 'base_model.model.visual.blocks.17.mlp.fc1.bias', 'base_model.model.visual.blocks.17.mlp.fc2.weight', 'base_model.model.visual.blocks.17.mlp.fc2.bias', 'base_model.model.visual.blocks.18.norm1.weight', 'base_model.model.visual.blocks.18.norm1.bias', 'base_model.model.visual.blocks.18.attn.q_bias', 'base_model.model.visual.blocks.18.attn.v_bias', 'base_model.model.visual.blocks.18.attn.qkv.weight', 'base_model.model.visual.blocks.18.attn.proj.weight', 'base_model.model.visual.blocks.18.attn.proj.bias', 'base_model.model.visual.blocks.18.norm2.weight', 'base_model.model.visual.blocks.18.norm2.bias', 'base_model.model.visual.blocks.18.mlp.fc1.weight', 'base_model.model.visual.blocks.18.mlp.fc1.bias', 'base_model.model.visual.blocks.18.mlp.fc2.weight', 'base_model.model.visual.blocks.18.mlp.fc2.bias', 'base_model.model.visual.blocks.19.norm1.weight', 'base_model.model.visual.blocks.19.norm1.bias', 'base_model.model.visual.blocks.19.attn.q_bias', 'base_model.model.visual.blocks.19.attn.v_bias', 'base_model.model.visual.blocks.19.attn.qkv.weight', 'base_model.model.visual.blocks.19.attn.proj.weight', 'base_model.model.visual.blocks.19.attn.proj.bias', 'base_model.model.visual.blocks.19.norm2.weight', 'base_model.model.visual.blocks.19.norm2.bias', 'base_model.model.visual.blocks.19.mlp.fc1.weight', 'base_model.model.visual.blocks.19.mlp.fc1.bias', 'base_model.model.visual.blocks.19.mlp.fc2.weight', 'base_model.model.visual.blocks.19.mlp.fc2.bias', 'base_model.model.visual.blocks.20.norm1.weight', 'base_model.model.visual.blocks.20.norm1.bias', 'base_model.model.visual.blocks.20.attn.q_bias', 'base_model.model.visual.blocks.20.attn.v_bias', 'base_model.model.visual.blocks.20.attn.qkv.weight', 'base_model.model.visual.blocks.20.attn.proj.weight', 'base_model.model.visual.blocks.20.attn.proj.bias', 'base_model.model.visual.blocks.20.norm2.weight', 'base_model.model.visual.blocks.20.norm2.bias', 'base_model.model.visual.blocks.20.mlp.fc1.weight', 'base_model.model.visual.blocks.20.mlp.fc1.bias', 'base_model.model.visual.blocks.20.mlp.fc2.weight', 'base_model.model.visual.blocks.20.mlp.fc2.bias', 'base_model.model.visual.blocks.21.norm1.weight', 'base_model.model.visual.blocks.21.norm1.bias', 'base_model.model.visual.blocks.21.attn.q_bias', 'base_model.model.visual.blocks.21.attn.v_bias', 'base_model.model.visual.blocks.21.attn.qkv.weight', 'base_model.model.visual.blocks.21.attn.proj.weight', 'base_model.model.visual.blocks.21.attn.proj.bias', 'base_model.model.visual.blocks.21.norm2.weight', 'base_model.model.visual.blocks.21.norm2.bias', 'base_model.model.visual.blocks.21.mlp.fc1.weight', 'base_model.model.visual.blocks.21.mlp.fc1.bias', 'base_model.model.visual.blocks.21.mlp.fc2.weight', 'base_model.model.visual.blocks.21.mlp.fc2.bias', 'base_model.model.visual.blocks.22.norm1.weight', 'base_model.model.visual.blocks.22.norm1.bias', 'base_model.model.visual.blocks.22.attn.q_bias', 'base_model.model.visual.blocks.22.attn.v_bias', 'base_model.model.visual.blocks.22.attn.qkv.weight', 'base_model.model.visual.blocks.22.attn.proj.weight', 'base_model.model.visual.blocks.22.attn.proj.bias', 'base_model.model.visual.blocks.22.norm2.weight', 'base_model.model.visual.blocks.22.norm2.bias', 'base_model.model.visual.blocks.22.mlp.fc1.weight', 'base_model.model.visual.blocks.22.mlp.fc1.bias', 'base_model.model.visual.blocks.22.mlp.fc2.weight', 'base_model.model.visual.blocks.22.mlp.fc2.bias', 'base_model.model.visual.blocks.23.norm1.weight', 'base_model.model.visual.blocks.23.norm1.bias', 'base_model.model.visual.blocks.23.attn.q_bias', 'base_model.model.visual.blocks.23.attn.v_bias', 'base_model.model.visual.blocks.23.attn.qkv.weight', 'base_model.model.visual.blocks.23.attn.proj.weight', 'base_model.model.visual.blocks.23.attn.proj.bias', 'base_model.model.visual.blocks.23.norm2.weight', 'base_model.model.visual.blocks.23.norm2.bias', 'base_model.model.visual.blocks.23.mlp.fc1.weight', 'base_model.model.visual.blocks.23.mlp.fc1.bias', 'base_model.model.visual.blocks.23.mlp.fc2.weight', 'base_model.model.visual.blocks.23.mlp.fc2.bias', 'base_model.model.visual.blocks.24.norm1.weight', 'base_model.model.visual.blocks.24.norm1.bias', 'base_model.model.visual.blocks.24.attn.q_bias', 'base_model.model.visual.blocks.24.attn.v_bias', 'base_model.model.visual.blocks.24.attn.qkv.weight', 'base_model.model.visual.blocks.24.attn.proj.weight', 'base_model.model.visual.blocks.24.attn.proj.bias', 'base_model.model.visual.blocks.24.norm2.weight', 'base_model.model.visual.blocks.24.norm2.bias', 'base_model.model.visual.blocks.24.mlp.fc1.weight', 'base_model.model.visual.blocks.24.mlp.fc1.bias', 'base_model.model.visual.blocks.24.mlp.fc2.weight', 'base_model.model.visual.blocks.24.mlp.fc2.bias', 'base_model.model.visual.blocks.25.norm1.weight', 'base_model.model.visual.blocks.25.norm1.bias', 'base_model.model.visual.blocks.25.attn.q_bias', 'base_model.model.visual.blocks.25.attn.v_bias', 'base_model.model.visual.blocks.25.attn.qkv.weight', 'base_model.model.visual.blocks.25.attn.proj.weight', 'base_model.model.visual.blocks.25.attn.proj.bias', 'base_model.model.visual.blocks.25.norm2.weight', 'base_model.model.visual.blocks.25.norm2.bias', 'base_model.model.visual.blocks.25.mlp.fc1.weight', 'base_model.model.visual.blocks.25.mlp.fc1.bias', 'base_model.model.visual.blocks.25.mlp.fc2.weight', 'base_model.model.visual.blocks.25.mlp.fc2.bias', 'base_model.model.visual.blocks.26.norm1.weight', 'base_model.model.visual.blocks.26.norm1.bias', 'base_model.model.visual.blocks.26.attn.q_bias', 'base_model.model.visual.blocks.26.attn.v_bias', 'base_model.model.visual.blocks.26.attn.qkv.weight', 'base_model.model.visual.blocks.26.attn.proj.weight', 'base_model.model.visual.blocks.26.attn.proj.bias', 'base_model.model.visual.blocks.26.norm2.weight', 'base_model.model.visual.blocks.26.norm2.bias', 'base_model.model.visual.blocks.26.mlp.fc1.weight', 'base_model.model.visual.blocks.26.mlp.fc1.bias', 'base_model.model.visual.blocks.26.mlp.fc2.weight', 'base_model.model.visual.blocks.26.mlp.fc2.bias', 'base_model.model.visual.blocks.27.norm1.weight', 'base_model.model.visual.blocks.27.norm1.bias', 'base_model.model.visual.blocks.27.attn.q_bias', 'base_model.model.visual.blocks.27.attn.v_bias', 'base_model.model.visual.blocks.27.attn.qkv.weight', 'base_model.model.visual.blocks.27.attn.proj.weight', 'base_model.model.visual.blocks.27.attn.proj.bias', 'base_model.model.visual.blocks.27.norm2.weight', 'base_model.model.visual.blocks.27.norm2.bias', 'base_model.model.visual.blocks.27.mlp.fc1.weight', 'base_model.model.visual.blocks.27.mlp.fc1.bias', 'base_model.model.visual.blocks.27.mlp.fc2.weight', 'base_model.model.visual.blocks.27.mlp.fc2.bias', 'base_model.model.visual.blocks.28.norm1.weight', 'base_model.model.visual.blocks.28.norm1.bias', 'base_model.model.visual.blocks.28.attn.q_bias', 'base_model.model.visual.blocks.28.attn.v_bias', 'base_model.model.visual.blocks.28.attn.qkv.weight', 'base_model.model.visual.blocks.28.attn.proj.weight', 'base_model.model.visual.blocks.28.attn.proj.bias', 'base_model.model.visual.blocks.28.norm2.weight', 'base_model.model.visual.blocks.28.norm2.bias', 'base_model.model.visual.blocks.28.mlp.fc1.weight', 'base_model.model.visual.blocks.28.mlp.fc1.bias', 'base_model.model.visual.blocks.28.mlp.fc2.weight', 'base_model.model.visual.blocks.28.mlp.fc2.bias', 'base_model.model.visual.blocks.29.norm1.weight', 'base_model.model.visual.blocks.29.norm1.bias', 'base_model.model.visual.blocks.29.attn.q_bias', 'base_model.model.visual.blocks.29.attn.v_bias', 'base_model.model.visual.blocks.29.attn.qkv.weight', 'base_model.model.visual.blocks.29.attn.proj.weight', 'base_model.model.visual.blocks.29.attn.proj.bias', 'base_model.model.visual.blocks.29.norm2.weight', 'base_model.model.visual.blocks.29.norm2.bias', 'base_model.model.visual.blocks.29.mlp.fc1.weight', 'base_model.model.visual.blocks.29.mlp.fc1.bias', 'base_model.model.visual.blocks.29.mlp.fc2.weight', 'base_model.model.visual.blocks.29.mlp.fc2.bias', 'base_model.model.visual.blocks.30.norm1.weight', 'base_model.model.visual.blocks.30.norm1.bias', 'base_model.model.visual.blocks.30.attn.q_bias', 'base_model.model.visual.blocks.30.attn.v_bias', 'base_model.model.visual.blocks.30.attn.qkv.weight', 'base_model.model.visual.blocks.30.attn.proj.weight', 'base_model.model.visual.blocks.30.attn.proj.bias', 'base_model.model.visual.blocks.30.norm2.weight', 'base_model.model.visual.blocks.30.norm2.bias', 'base_model.model.visual.blocks.30.mlp.fc1.weight', 'base_model.model.visual.blocks.30.mlp.fc1.bias', 'base_model.model.visual.blocks.30.mlp.fc2.weight', 'base_model.model.visual.blocks.30.mlp.fc2.bias', 'base_model.model.visual.blocks.31.norm1.weight', 'base_model.model.visual.blocks.31.norm1.bias', 'base_model.model.visual.blocks.31.attn.q_bias', 'base_model.model.visual.blocks.31.attn.v_bias', 'base_model.model.visual.blocks.31.attn.qkv.weight', 'base_model.model.visual.blocks.31.attn.proj.weight', 'base_model.model.visual.blocks.31.attn.proj.bias', 'base_model.model.visual.blocks.31.norm2.weight', 'base_model.model.visual.blocks.31.norm2.bias', 'base_model.model.visual.blocks.31.mlp.fc1.weight', 'base_model.model.visual.blocks.31.mlp.fc1.bias', 'base_model.model.visual.blocks.31.mlp.fc2.weight', 'base_model.model.visual.blocks.31.mlp.fc2.bias', 'base_model.model.visual.blocks.32.norm1.weight', 'base_model.model.visual.blocks.32.norm1.bias', 'base_model.model.visual.blocks.32.attn.q_bias', 'base_model.model.visual.blocks.32.attn.v_bias', 'base_model.model.visual.blocks.32.attn.qkv.weight', 'base_model.model.visual.blocks.32.attn.proj.weight', 'base_model.model.visual.blocks.32.attn.proj.bias', 'base_model.model.visual.blocks.32.norm2.weight', 'base_model.model.visual.blocks.32.norm2.bias', 'base_model.model.visual.blocks.32.mlp.fc1.weight', 'base_model.model.visual.blocks.32.mlp.fc1.bias', 'base_model.model.visual.blocks.32.mlp.fc2.weight', 'base_model.model.visual.blocks.32.mlp.fc2.bias', 'base_model.model.visual.blocks.33.norm1.weight', 'base_model.model.visual.blocks.33.norm1.bias', 'base_model.model.visual.blocks.33.attn.q_bias', 'base_model.model.visual.blocks.33.attn.v_bias', 'base_model.model.visual.blocks.33.attn.qkv.weight', 'base_model.model.visual.blocks.33.attn.proj.weight', 'base_model.model.visual.blocks.33.attn.proj.bias', 'base_model.model.visual.blocks.33.norm2.weight', 'base_model.model.visual.blocks.33.norm2.bias', 'base_model.model.visual.blocks.33.mlp.fc1.weight', 'base_model.model.visual.blocks.33.mlp.fc1.bias', 'base_model.model.visual.blocks.33.mlp.fc2.weight', 'base_model.model.visual.blocks.33.mlp.fc2.bias', 'base_model.model.visual.blocks.34.norm1.weight', 'base_model.model.visual.blocks.34.norm1.bias', 'base_model.model.visual.blocks.34.attn.q_bias', 'base_model.model.visual.blocks.34.attn.v_bias', 'base_model.model.visual.blocks.34.attn.qkv.weight', 'base_model.model.visual.blocks.34.attn.proj.weight', 'base_model.model.visual.blocks.34.attn.proj.bias', 'base_model.model.visual.blocks.34.norm2.weight', 'base_model.model.visual.blocks.34.norm2.bias', 'base_model.model.visual.blocks.34.mlp.fc1.weight', 'base_model.model.visual.blocks.34.mlp.fc1.bias', 'base_model.model.visual.blocks.34.mlp.fc2.weight', 'base_model.model.visual.blocks.34.mlp.fc2.bias', 'base_model.model.visual.blocks.35.norm1.weight', 'base_model.model.visual.blocks.35.norm1.bias', 'base_model.model.visual.blocks.35.attn.q_bias', 'base_model.model.visual.blocks.35.attn.v_bias', 'base_model.model.visual.blocks.35.attn.qkv.weight', 'base_model.model.visual.blocks.35.attn.proj.weight', 'base_model.model.visual.blocks.35.attn.proj.bias', 'base_model.model.visual.blocks.35.norm2.weight', 'base_model.model.visual.blocks.35.norm2.bias', 'base_model.model.visual.blocks.35.mlp.fc1.weight', 'base_model.model.visual.blocks.35.mlp.fc1.bias', 'base_model.model.visual.blocks.35.mlp.fc2.weight', 'base_model.model.visual.blocks.35.mlp.fc2.bias', 'base_model.model.visual.blocks.36.norm1.weight', 'base_model.model.visual.blocks.36.norm1.bias', 'base_model.model.visual.blocks.36.attn.q_bias', 'base_model.model.visual.blocks.36.attn.v_bias', 'base_model.model.visual.blocks.36.attn.qkv.weight', 'base_model.model.visual.blocks.36.attn.proj.weight', 'base_model.model.visual.blocks.36.attn.proj.bias', 'base_model.model.visual.blocks.36.norm2.weight', 'base_model.model.visual.blocks.36.norm2.bias', 'base_model.model.visual.blocks.36.mlp.fc1.weight', 'base_model.model.visual.blocks.36.mlp.fc1.bias', 'base_model.model.visual.blocks.36.mlp.fc2.weight', 'base_model.model.visual.blocks.36.mlp.fc2.bias', 'base_model.model.visual.blocks.37.norm1.weight', 'base_model.model.visual.blocks.37.norm1.bias', 'base_model.model.visual.blocks.37.attn.q_bias', 'base_model.model.visual.blocks.37.attn.v_bias', 'base_model.model.visual.blocks.37.attn.qkv.weight', 'base_model.model.visual.blocks.37.attn.proj.weight', 'base_model.model.visual.blocks.37.attn.proj.bias', 'base_model.model.visual.blocks.37.norm2.weight', 'base_model.model.visual.blocks.37.norm2.bias', 'base_model.model.visual.blocks.37.mlp.fc1.weight', 'base_model.model.visual.blocks.37.mlp.fc1.bias', 'base_model.model.visual.blocks.37.mlp.fc2.weight', 'base_model.model.visual.blocks.37.mlp.fc2.bias', 'base_model.model.visual.blocks.38.norm1.weight', 'base_model.model.visual.blocks.38.norm1.bias', 'base_model.model.visual.blocks.38.attn.q_bias', 'base_model.model.visual.blocks.38.attn.v_bias', 'base_model.model.visual.blocks.38.attn.qkv.weight', 'base_model.model.visual.blocks.38.attn.proj.weight', 'base_model.model.visual.blocks.38.attn.proj.bias', 'base_model.model.visual.blocks.38.norm2.weight', 'base_model.model.visual.blocks.38.norm2.bias', 'base_model.model.visual.blocks.38.mlp.fc1.weight', 'base_model.model.visual.blocks.38.mlp.fc1.bias', 'base_model.model.visual.blocks.38.mlp.fc2.weight', 'base_model.model.visual.blocks.38.mlp.fc2.bias', 'base_model.model.visual.blocks.39.norm1.weight', 'base_model.model.visual.blocks.39.norm1.bias', 'base_model.model.visual.blocks.39.attn.q_bias', 'base_model.model.visual.blocks.39.attn.v_bias', 'base_model.model.visual.blocks.39.attn.qkv.weight', 'base_model.model.visual.blocks.39.attn.proj.weight', 'base_model.model.visual.blocks.39.attn.proj.bias', 'base_model.model.visual.blocks.39.norm2.weight', 'base_model.model.visual.blocks.39.norm2.bias', 'base_model.model.visual.blocks.39.mlp.fc1.weight', 'base_model.model.visual.blocks.39.mlp.fc1.bias', 'base_model.model.visual.blocks.39.mlp.fc2.weight', 'base_model.model.visual.blocks.39.mlp.fc2.bias', 'base_model.model.visual.norm.weight', 'base_model.model.visual.norm.bias', 'base_model.model.visual.head.weight', 'base_model.model.visual.head.bias', 'base_model.model.ln_visual.weight', 'base_model.model.ln_visual.bias', 'base_model.model.cformer.causal_tokens', 'base_model.model.cformer.cformer.embed_tokens.weight', 'base_model.model.cformer.cformer.block.0.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.0.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.0.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.0.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'base_model.model.cformer.cformer.block.0.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.0.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.0.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.0.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.0.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.0.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.0.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.0.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.0.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.1.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.1.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.1.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.1.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.1.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.1.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.1.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.1.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.1.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.1.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.1.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.1.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.1.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.2.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.2.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.2.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.2.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.2.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.2.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.2.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.2.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.2.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.2.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.2.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.2.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.2.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.3.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.3.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.3.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.3.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.3.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.3.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.3.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.3.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.3.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.3.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.3.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.3.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.3.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.4.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.4.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.4.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.4.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.4.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.4.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.4.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.4.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.4.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.4.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.4.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.4.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.4.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.5.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.5.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.5.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.5.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.5.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.5.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.5.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.5.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.5.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.5.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.5.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.5.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.5.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.6.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.6.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.6.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.6.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.6.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.6.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.6.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.6.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.6.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.6.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.6.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.6.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.6.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.7.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.7.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.7.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.7.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.7.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.7.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.7.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.7.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.7.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.7.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.7.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.7.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.7.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.8.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.8.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.8.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.8.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.8.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.8.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.8.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.8.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.8.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.8.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.8.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.8.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.8.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.9.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.9.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.9.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.9.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.9.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.9.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.9.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.9.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.9.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.9.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.9.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.9.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.9.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.10.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.10.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.10.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.10.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.10.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.10.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.10.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.10.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.10.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.10.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.10.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.10.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.10.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.block.11.layer.0.SelfAttention.q.weight', 'base_model.model.cformer.cformer.block.11.layer.0.SelfAttention.k.weight', 'base_model.model.cformer.cformer.block.11.layer.0.SelfAttention.v.weight', 'base_model.model.cformer.cformer.block.11.layer.0.SelfAttention.o.weight', 'base_model.model.cformer.cformer.block.11.layer.0.layer_norm.weight', 'base_model.model.cformer.cformer.block.11.layer.1.EncDecAttention.q.weight', 'base_model.model.cformer.cformer.block.11.layer.1.EncDecAttention.k.weight', 'base_model.model.cformer.cformer.block.11.layer.1.EncDecAttention.v.weight', 'base_model.model.cformer.cformer.block.11.layer.1.EncDecAttention.o.weight', 'base_model.model.cformer.cformer.block.11.layer.1.layer_norm.weight', 'base_model.model.cformer.cformer.block.11.layer.2.DenseReluDense.wi.weight', 'base_model.model.cformer.cformer.block.11.layer.2.DenseReluDense.wo.weight', 'base_model.model.cformer.cformer.block.11.layer.2.layer_norm.weight', 'base_model.model.cformer.cformer.final_layer_norm.weight', 'base_model.model.cformer.projection.weight', 'base_model.model.cformer.projection.bias'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e8e98e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaNUWA(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32006, 5120)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (8): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (9): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (10): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (11): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (16): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (17): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (18): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (19): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (20): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (21): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (22): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (23): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (24): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (25): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (26): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (27): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (28): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (29): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (30): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (31): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (32): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (33): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (34): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (35): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (36): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (37): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (38): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "          (39): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=32006, bias=False)\n",
       "      (visual): EVAVisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (24): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (25): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (26): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (27): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (28): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (29): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (30): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (31): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (32): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (33): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (34): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (35): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (36): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (37): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (38): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (39): Block(\n",
       "            (norm1): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (inner_attn_ln): Identity()\n",
       "              (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (ffn_ln): Identity()\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNormFp32((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        (head): Linear(in_features=1408, out_features=1024, bias=True)\n",
       "        (patch_dropout): Identity()\n",
       "      )\n",
       "      (ln_visual): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "      (cformer): CausalFormer(\n",
       "        (cformer): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (5): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (6): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (7): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (8): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (9): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (10): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (11): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (k): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (v): Linear(in_features=1408, out_features=768, bias=False)\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (projection): Linear(in_features=768, out_features=5120, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "470d7604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0002, -0.0001,  0.0008,  ...,  0.0008, -0.0005,  0.0006]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model.embed_tokens(torch.LongTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f3fed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0002, -0.0001,  0.0008,  ...,  0.0008, -0.0005,  0.0006]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.embed_tokens(torch.LongTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a7303ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.embed_tokens.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.embed_tokens.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model.embed_tokens.weight'"
     ]
    }
   ],
   "source": [
    "model.state_dict()['model.embed_tokens.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "364d732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32006, 5120])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dicts['base_model.model.model.embed_tokens.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape not Align"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamax",
   "language": "python",
   "name": "llamax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
